dfm_count<-dfm_df %>%
pivot_longer(-doc_id,names_to = "word", values_to = "n")%>%
group_by(word)%>%
summarise(Frequency=sum(n))
# with library(FactoMineR)
foo<- dfm_df %>%
column_to_rownames(var="doc_id")
#PCA
res.pca <- PCA(foo[,c(1:1658)], ncp=10,graph=FALSE) #could be pretty long
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 10))
#label selection factor
a=.2
foo1<-as.data.frame(res.pca$var$coord) %>%
rownames_to_column(var="label") %>%
filter(Dim.1>a | Dim.1< -a | Dim.2>a | Dim.2< - a)
ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
geom_text(aes(label=label),size=2)
foo1<-as.data.frame(res.pca$var$coord) %>%
rownames_to_column(var="label") %>%
filter(Dim.3> a | Dim.3< -a | Dim.4> a | Dim.4< -a)
ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
geom_text(aes(label=label),size=2)
#typo with kmnes
foo<-as.data.frame(res.pca$var$coord)
resKM <- kmeans(foo, 20, nstart = 25, trace=0)
foo<-as.data.frame(resKM$cluster) %>%
rename(cluster=1)%>%
rownames_to_column(var="word") %>%
left_join(dfm_count)
foo$cluster<-as.factor(foo$cluster)
set.seed(42) #for reproducibility
#library(ggwordcloud)
ggplot(foo, aes(label = word, size=Frequency, group = cluster)) +
geom_text_wordcloud() +
scale_size_area(max_size = 5) +
facet_wrap(vars(cluster), ncol=4)+
theme_minimal()+
labs(title=NULL) #comment enlever la numérotation ?
ggsave("./Images/cluster.jpeg", width = 28, height = 20, units = "cm")
#we need to transpose the matrix
foo<-dfm_df %>%
select(-doc_id)%>%t()
#just to keep words aside
word<-as.data.frame(rownames(foo))%>%
rename(word=1)
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo,
initial_dims = 50,
perplexity = 50,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1)
tsne_out1<-as.data.frame(tsne_out$Y)
tsne_out2<-cbind(word,tsne_out1)%>%
left_join(dfm_count)%>%
filter(Frequency<7000 & Frequency>150)
ggplot(tsne_out2, aes(x=V1, y=V2))+
geom_text_repel(aes(label=word,
size=log10(Frequency),
alpha=log10(Frequency)),
color="black",
max.overlap=Inf)+
theme(legend.position = "none")+
labs(x=NULL, y=NULL)+
scale_size(range = c(.1, 3.5))
ggsave("./Images/tsne.jpeg", width = 28, height = 20, units = "cm")
##coocurrence computing
tag_fcm <- fcm(dfm)
head(tag_fcm)
#select top tags
toptag <- names(topfeatures(dfm, 100))
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) #select 500 more frequent words
textplot_network(topgat_fcm,
min_freq = 0.5,
edge_alpha = 0.01,
edge_size = 0.1,
vertex_labelsize = 2.5)
ggsave("./Images/network.jpeg", width = 28, height = 20, units = "cm")
foo<-UD %>%
filter(upos=="NOUN"|upos=="PROPN"|upos=="VERB"|upos=="ADJ")%>%
group_by(doc_id)%>%
count(doc_id, lemma, sort=TRUE)
total_words <- foo %>%
group_by(doc_id) %>%
summarize(total = sum(n))
Avis_words <- left_join(foo, total_words) %>%
mutate(Term_frequency=n/total)%>%
group_by(doc_id)%>%
summarise(word=lemma,rank = row_number(),
n=n,
Term_frequency=Term_frequency)
Avis_words
Avis_words%>%
filter(doc_id=="doc12441"|doc_id=="doc12912") %>%
ggplot(aes(x=Term_frequency, fill = doc_id)) +
geom_histogram(show.legend = FALSE) +
#  xlim(NA, 0.0009) +
facet_wrap(~doc_id, ncol = 2, scales = "free_y")
Avis_words%>%
filter(doc_id=="doc12441"|doc_id=="doc12912") %>%
ggplot(aes(x=rank,Term_frequency, color = doc_id)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
Avis_tf_idf <- Avis_words %>%
bind_tf_idf(word, doc_id, n)
Avis_tf_idf %>%
select(-rank,n,Term_frequency) %>%
filter(tf_idf<0.5)%>%
arrange(desc(tf_idf))
Avis_tf_idf%>%
filter(doc_id=="doc12441"|doc_id=="doc12912") %>%
group_by(doc_id) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = doc_id)) +
geom_col(show.legend = FALSE) +
facet_wrap(~doc_id, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
Avis_tf_idf%>%
ggplot(aes(tf_idf))+
geom_histogram(binwidth = 0.002)+
xlim(0,1)
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo,
initial_dims = 10,
perplexity = 30,
theta=.5,
num_threads=4, verbose=1, is_distance=TRUE)
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo,
initial_dims = 10,
perplexity = 30,
theta=.5,
num_threads=4, verbose=1)
View(foo)
foo<-Avis_tf_idf %>%
left_join(dfm_count) %>%
filter(Frequency>50)%>%
select(doc_id, word, tf_idf) %>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
column_to_rownames(var="doc_id")
tsne_out <- Rtsne(foo,
initial_dims = 10,
perplexity = 30,
theta=.5,
num_threads=4, verbose=1)
View(foo)
foo<-Avis_tf_idf %>%
left_join(dfm_count) %>%
filter(Frequency>50)%>%
select(doc_id, word, tf_idf) %>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
column_to_rownames(var="doc_id") %>%
replace_na(0)
foo<-Avis_tf_idf %>%
left_join(dfm_count) %>%
filter(Frequency>50)%>%
select(doc_id, word, tf_idf) %>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
column_to_rownames(var="doc_id") %>%
mutate(
across(everything(), ~replace_na(.x, 0))
)
tsne_out <- Rtsne(foo,
initial_dims = 10,
perplexity = 30,
theta=.5,
num_threads=4, verbose=1)
tsne_out <- Rtsne(initial_dims = 50,
perplexity = 50,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1
check_duplicates = FALSE)
tsne_out <- Rtsne(initial_dims = 50,
perplexity = 50,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out <- Rtsne(foo,
initial_dims = 50,
perplexity = 50,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1) )%>%
left_join(dfm_count)%>%
filter(Frequency<7000 & Frequency>100)
foo<-t(foo)
tsne_out <- Rtsne(foo,
initial_dims = 50,
perplexity = 50,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1) )%>%
left_join(dfm_count)%>%
filter(Frequency<7000 & Frequency>100)
View(tsne_out1)
foo<-Avis_tf_idf %>%
left_join(dfm_count) %>%
filter(Frequency>50)%>%
select(doc_id, word, tf_idf) %>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
column_to_rownames(var="doc_id") %>%
mutate(
across(everything(), ~replace_na(.x, 0))
)
#just to keep words aside
word<-as.data.frame(rownames(foo))%>%
rename(word=1)
foo<-t(foo)
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1) )%>%
left_join(dfm_count)%>%
filter(Frequency<7000 & Frequency>100)
foo<-t(foo)
foo<-Avis_tf_idf %>%
left_join(dfm_count) %>%
filter(Frequency>50)%>%
select(doc_id, word, tf_idf) %>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
column_to_rownames(var="doc_id") %>%
mutate(
across(everything(), ~replace_na(.x, 0))
)
#just to keep words aside
foo<-t(foo)
word<-as.data.frame(rownames(foo))%>%
rename(word=1)
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1) )%>%
left_join(dfm_count)%>%
filter(Frequency<7000 & Frequency>100)
ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
geom_text_repel(aes(label=word,
size=log10(Frequency),
alpha=log10(Frequency)),
color="black",
max.overlap=Inf)+
theme(legend.position = "none")+
labs(x=NULL, y=NULL)+
scale_size(range = c(.1, 3))
ggsave("./Images/tsne2.jpeg", width = 28, height = 20, units = "cm")
# 1 corpus definition
df_work<-readRDS("./Data/df_work.rds")
corpus<-corpus(df_work,text_field ="description")
![]("LDA-algorithm.png")
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
#from session 1 & 2
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(syuzhet)  #analyse du sentimeent
# new for session 2
library(FactoMineR)
library(factoextra)
library(igraph)
library(ggwordcloud)
library(ggrepel)
library(Rtsne)
library(tidytext)
# new for session 3
library()
theme_set(theme_minimal())
t1=Sys.time()
df<-readRDS("./Data/UD.rds")
tf <- UD$lemma %>%
filter(upos %in% c("ADJ", "PROPN","NOUN","VERB")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
library(cleanNLP)
tf <- UD$lemma %>%
filter(upos %in% c("ADJ", "PROPN","NOUN","VERB")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
View(UD)
tf <- UD%>%
filter(upos %in% c("ADJ", "PROPN","NOUN","VERB")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
library(text2vec)
lda_model = LDA$new(n_topics = 8, doc_topic_prior = 0.1, topic_word_prior = 0.01)
lda_model = LDA$new(n_topics = 24, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproductibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = FALSE)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")+
theme_light()
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("ADJ", "PROPN","NOUN","VERB")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 24, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproductibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")+
theme_light()
library(LDAvis)
lda_model$plot() mode interactif
lda_model$plot() #mode interactif
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("ADJ", "PROPN","NOUN","VERB")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 12, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproductibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")+
theme_light()
library(LDAvis)
lda_model$plot() #mode interactif
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")+
theme_light()
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("PROPN","NOUN")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 12, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproductibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")
library(LDAvis)
#lda_model$plot() #mode interactif
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("ADJ","ADV")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 12, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproducibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+
scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("ADJ")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 12, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproducibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("ADJ", "ADV")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 12, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproducibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf, n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+
scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")
df<-readRDS("./Data/UD.rds")
#library(cleanNLP) #an other technics for tf_idf
tf <- UD%>%
filter(upos %in% c("NOUN", "VERB")) %>%
cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, tf_weight = "raw")
#library(text2vec)
lda_model = LDA$new(n_topics = 12, doc_topic_prior = 0.1, topic_word_prior = 0.01)
set.seed(67) #pour la reproducibilité des résultats
#On définit les paramètres du processus d'estimation :
##n_iter = le nombre d'itérations
##convergence_tol =le seuil de convergence
doc_topic_distr =
lda_model$fit_transform(x = tf,
n_iter = 1000,
convergence_tol = 0.001,
n_check_convergence = 25,
progressbar = TRUE)
#description des topics en fonction d'un degré de pertinence = lamba ( lambda =1 probabilité d'obtenir le terme sachant le topic)
lda_res<-as.data.frame(lda_model$get_top_words(n = 15, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-lda_res%>% gather(variable, value, -rank)
ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) +
scale_y_reverse() +
geom_text(aes(color=variable,size=sqrt(26-rank)))+
scale_color_hue()+
guides(color="none",size="none")+
labs(x="topics", y="par ordre de pertinence")
# un nuage de mot plutôt
library(LDAvis)
#lda_model$plot() #mode interactif
lda_model$plot() #mode interactif
