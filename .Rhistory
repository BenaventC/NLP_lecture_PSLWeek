library(cowplot) #for ploting
library(quanteda) #a complete text mining package
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library("quanteda.dictionaries")
library(syuzhet)  #analyse du sentimeent
library(lubridate) #date processing
library(udpipe) #annotations
library(igraph) #représentation de réseaux
theme_set(theme_minimal())
t1=Sys.time()
#read the file and sample to reduce computation ( look at the end)
df <- read_csv("data_original_12062022_normalise_sentiment_transformers_lemna_words_adj.csv")%>%
select (id, date_ecrit, titre, description, ressenti, pays,intitule, canaux_typologie_1, transformer_sentiment_score)
# un peu de correction
df<-df %>%
mutate(description=str_replace_all(description,"\\.(?=[A-Za-z])", ". "),
description=str_replace(description,"_", " "))
#%>% sample_n(1000)
head(df, 5)
g1<-ggplot(df,aes(x=transformer_sentiment_score))+
geom_density(fill="pink", alpha=.5)
g2<-ggplot(df, aes(x=transformer_sentiment_score)) +
stat_ecdf(geom = "step",pad = FALSE)+
xlim(0,1)
#cowplot
plot_grid(
g1, g2,
labels = "AUTO"
)
#score et ressenti
g2<-ggplot(df, aes(y=transformer_sentiment_score, x=ressenti)) +
geom_violin(fill="pink", alpha=.5)+
ylim(0,1)+
geom_smooth()+
labs(x=NULL, y = "Score transformer")
g2
## multiple choice question recoding
typo<-as.data.frame(str_split_fixed(df$canaux_typologie_1, ",", 10))%>%
mutate(across(where(is.character), str_trim)) %>%
cbind(df$id) %>%
rename(id=11) %>%
pivot_longer(-id, names_to = "valeur", values_to="variable") %>%
filter(variable!="") %>%
group_by(id, variable) %>%
summarise(valeur=1) %>%
pivot_wider(id,names_from="variable", values_from="valeur")%>%
replace(is.na(.), 0)
#count the number of row
n_c<-nrow(typo)
n_c
foo<-typo %>%
pivot_longer(-id, names_to = "variable", values_to="value") %>%
group_by(variable)%>%
summarise(Penetration=mean(value))
ggplot(foo, aes(x=reorder(variable, Penetration),y=Penetration))+
geom_bar(stat="identity",fill="firebrick")+
coord_flip()+
labs(x=NULL, y="taux de pénétration")
#typo
foo<-typo%>%
select(-id)
#service
t<-as.data.frame(table(df$intitule)) %>%
filter(Freq>50)
ggplot(t, aes(x=reorder(Var1, Freq),y=Freq))+
geom_bar(stat="identity",fill="firebrick")+
coord_flip()+scale_y_log10()
foo<-df%>%mutate(n=1)%>%
group_by(intitule)%>%
summarise(score=mean(transformer_sentiment_score,na.rm=TRUE),n=sum(n)) %>%
filter(n>50)
ggplot(foo, aes(x=reorder(intitule, n), y=score ))+  geom_bar(stat="identity",fill="firebrick")+
coord_flip()
df$n_words<-str_count(df$description)
ggplot(df,aes(x=n_words))+
geom_density()+scale_x_log10()
foo<-df %>%
group_by(date_ecrit)%>%
summarise(n=n(),
size_t=mean(n_words))
ggplot(foo, aes(x=date_ecrit, y=n))+
geom_point()+
scale_y_log10()+
geom_smooth()
ggplot(foo, aes(x=date_ecrit, y=size_t))+
geom_point()+
scale_y_log10()+
geom_smooth()
ggplot(foo, aes(x=n, y=size_t))+
geom_point()+
scale_x_log10()+  scale_y_log10()+
geom_smooth()
#la fonction de calcul de lisibilité
readability<-textstat_readability(df$description,
measure = c("Flesch",
"meanSentenceLength",
"meanWordSyllables"))
foo<-cbind(df[,2],readability[,2:4])
foo$date<-as.POSIXct(foo$date)
foo1<-foo %>%
dplyr::mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
select(-Month, -date_ecrit,-Year)%>%
group_by(date) %>%
summarise(Flesch=mean(Flesch, na.rm=TRUE),
SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
WordSyllables= mean(meanWordSyllables, na.rm=TRUE))
foo2<-foo1 %>%
pivot_longer(-date,names_to="Variable", values_to="Score")%>%
drop_na()
ggplot(foo2,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Experience readability", x=NULL, y=NULL)
lexdiv<-tokens(df$description)%>%
textstat_lexdiv(df$text, measure = c("CTTR", "Maas"),  log.base = 10,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE)
foo<-cbind(df,lexdiv[,2:5])
foo1<-foo %>% mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(CTTR=mean(CTTR, na.rm=TRUE),
Maas= mean(Maas, na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Lexical diversity", x=NULL, y=NULL)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(CTTR=mean(CTTR, na.rm=TRUE),
Maas= mean(Maas, na.rm=TRUE))
cor(foo1$CTTR, foo1$Maas)
ggplot(foo1,aes(x=CTTR, y=Maas))+
geom_point(size=1.2, aes(color=date), stat="identity")+
labs(title = "Lexical diversity", x=NULL, y=NULL)+geom_smooth(method="lm")
#library(syuzhet) analyse du sentimeent
#paramétres
method <- "nrc"
lang <- "french"
phrase<-as.character(paste0(df$titre,". ",df$description))
#extraction
emotions <- get_nrc_sentiment(phrase,language = "french")
emotion<-emotions[,1:8]
polarity<-subset(emotions,select=c(positive, negative))
foo<-cbind(df,polarity)%>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))
#mean per
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
mutate(positive=positive/n_words,
negative=negative/n_words)%>%
group_by(date) %>%
summarise(positive=mean(positive, na.rm=TRUE),
negative= -mean(negative, na.rm=TRUE),
valence=positive+negative,
expressivity=positive-negative) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#library("quanteda.dictionaries")
dict_liwc_french <- dictionary(file = "FrenchLIWCDictionary.dic",
format = "LIWC")
test<-liwcalike(df$description,dictionary = dict_liwc_french) %>%
select(je,vous, il, ils, pronomimp)
foo<-cbind(df,test)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(je=mean(je, na.rm=TRUE),
vous= mean(vous, na.rm=TRUE),
il_s=mean(il+ils,na.rm=TRUE),
pronomimp=mean(pronomimp,na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
my_text <- df$description
method <- "custom"
custom_lexicon <- data.frame(word=c("impot", "impôt","impots", "impôts", "taxe","taxes", "fisc", "fiscal", "fiscales", " fiscaux", "fiscalité", "redevance"),
value=c(1,1,1,1,1,1,1,1,1,1,1,1))
custom_distrib <- get_sentiment(my_text, method = method, lexicon = custom_lexicon)
custom_distrib<-as.data.frame(custom_distrib)
ggplot(custom_distrib,aes(x=custom_distrib))+geom_histogram()+scale_y_log10()
foo<-cbind(df,custom_distrib)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(custom_distrib=mean(custom_distrib, na.rm=TRUE))
ggplot(foo1,aes(x=date, y=custom_distrib))+
geom_line(size=1.2,stat="identity")+
labs(title = "Fisc", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#df$text<-str_replace(df$text, "\\w+", "J ") # trouver la solution!!!! pour le '
corpus<-corpus(df,id_field = "id",text_field = "description")
foo<-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)%>%
tokens_remove(stopwords("french"))
head(foo,5)
foo1 <-unlist_tokens(foo)
dim(foo1)
foo2<-foo1 %>%
group_by(token)%>%
summarise(n=n())%>%
mutate(rank=rank(desc(n)))
dim(foo2)
ggplot(foo2, aes(x=rank,y=n))+
geom_point(alpha=.2)+geom_smooth(method=lm)+
scale_x_log10()+
scale_y_log10()+
labs(title = "Zipf like")
#with cleaning
dfmat1 <- dfm(foo,
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat1, max_words = 50)
dfmat2 <- dfm(corpus_subset(corpus, intitule == "CAF"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 150)
#on sélectionne les mots commençant par une majuscule
toks_cap <- tokens_select(foo,
pattern = "^[A-Z]",
valuetype = "regex",
case_insensitive = FALSE,
padding = TRUE)
#on cherche les collocations
tstat_col_cap <- textstat_collocations(toks_cap, min_count = 3, tolower = FALSE)
#head(as.data.frame(tstat_col_cap),15)
toks_comp <- tokens_compound(foo, pattern = tstat_col_cap[tstat_col_cap$lamba > 10],
case_insensitive = FALSE)
head(toks_comp)
library(udpipe)
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")
UD <- udpipe_annotate(udmodel_french, x=df$description)
UD <- as.data.frame(UD)
saveRDS(UD, "UD.rds")
UD<-readRDS("UD.rds")
foo<-UD%>%
group_by(upos)%>%
summarise(n=n())%>%
ggplot(aes(x=reorder(upos,n), y=n))+
geom_bar(stat = "identity")+coord_flip()
foo
foo<-UD %>%
filter(upos=="NOUN") %>%
group_by(lemma)%>%
summarise(n=n()) %>%
filter(n>1000)%>%
ggplot(aes(x=reorder(lemma,n), y=n))+
geom_point(size=1.5, fill="blue3")+coord_flip()
foo
foo<-UD %>%
filter(upos=="ADJ" |upos=="VERB") %>%
group_by(lemma, upos)%>%
summarise(n=n()) %>%
filter(n>300)%>%
ggplot(aes(label = lemma, size = log(n), group=upos)) +
geom_text_wordcloud(aes(color=upos)) +
theme_minimal()+
facet_wrap(vars(upos))
foo
foo1<-UD %>%
mutate(id=paste0(doc_id,paragraph_id,sentence_id,token_id))%>%
select(id, lemma)%>%
rename(noun=lemma)
foo<-UD %>%
filter(dep_rel=="amod")%>%
mutate(id=paste0(doc_id,paragraph_id,sentence_id,head_token_id))%>%
left_join(foo1)%>%
select(id, lemma, noun)%>%
rename(adj=lemma)%>%
group_by(noun,adj)%>%
summarise(n=n())%>%
filter(n>50)
#A  Correspondance Analysis solution?
#igraph approach belong to the netx lesson
library(igraph)
g <- graph.data.frame(foo, directed=FALSE)
V(g)$type <- bipartite_mapping(g)$type  ## Add the "type" attribute
V(g)$label.color <- ifelse(V(g)$type, "salmon4", "blue2")
V(g)$fill <- ifelse(V(g)$type, "salmon4", "blue2")
V(g)$shape <-ifelse(V(g)$type, "circle", "square")
plot(g,vertex.label.cex = 0.8)
t2=Sys.time()
t<- t2-t1
print(t)
df_work<-cbind(df,readability, lexdiv,emotions,test)
write_rds(df_work,"df_work.rds")
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
#from session 1
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(syuzhet)  #analyse du sentimeent
# new for session 2
library(FactoMineR)
library(factoextra)
library(igraph)
library(ggwordcloud)
library(ggrepel)
library(Rtsne)
library(tidytext)
theme_set(theme_minimal())
t1=Sys.time()
#read the file and select tokens
df<-readRDS("UD.rds")%>%
select(doc_id, lemma, upos)%>%
filter(upos=="NOUN"|upos=="PROPN"|upos=="VERB"|upos=="ADJ")%>%
group_by(doc_id)%>%
summarise(text=paste(lemma, collapse=" "))
head(df,5)
#ajouter les variables doc_var
corpus <- corpus(df$text) # docvars(corpus, field = "doc_id")
summary(corpus)
dfm<- corpus %>%
tokens() %>% tokens_remove(stopwords("french"))%>%
dfm()
dfm<- dfm %>%
dfm_trim(min_termfreq = 100, verbose = FALSE)
set.seed(100)
textplot_wordcloud(dfm)
dfm_df<- as.data.frame(dfm)
dim(dfm_df)
head(dfm,5)
dfm_count<-dfm_df %>%
pivot_longer(-doc_id,names_to = "Word", values_to = "n")%>%
group_by(Word)%>% mutate(m=1)%>%
summarise(Doc_frequency=sum(m),  #une erreur à vérifier
Term_frequency=sum(n))
library(FactoMineR)
foo<- dfm_df %>%
column_to_rownames(var="doc_id")
#PCA
res.pca <- PCA(foo[,c(1:1030)], ncp=5,graph=FALSE)
#label selection factor
a=.3
foo1<-as.data.frame(res.pca$var$coord) %>%
rownames_to_column(var="label") %>%
filter(Dim.1>a | Dim.1< -a | Dim.2>a | Dim.2< - a)
ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
geom_text(aes(label=label),size=2)
foo1<-as.data.frame(res.pca$var$coord) %>%
rownames_to_column(var="label") %>%
filter(Dim.3> a | Dim.3< -a | Dim.4> a | Dim.4< -a)
ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
geom_text(aes(label=label),size=2)
#typo with kmnes
foo1<-as.data.frame(res.pca$var$coord)
resKM <- kmeans(foo1, 16, nstart = 10, trace=0)
foo<-as.data.frame(resKM$cluster) %>%
rename(cluster=1)%>%
rownames_to_column(var="Word") %>%
left_join(dfm_count)
foo$cluster<-as.factor(foo$cluster)
set.seed(42)
library(ggwordcloud)
ggplot(foo, aes(label = Word, size=Term_frequency, group = cluster)) +
geom_text_wordcloud() +
scale_size_area(max_size = 8) +
facet_wrap(vars(cluster), ncol=4)+theme_minimal()
dist<-dfm_df %>%
select(-doc_id)%>%
t()
Word<-as.data.frame(rownames(dist))%>%
rename(Word=1)
dist<-dist %>%
dist(method="canberra")
#library(Rtsne) # Load package
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(dist,
initial_dims = 10,
perplexity = 5,
theta=.5,
num_threads=4, verbose=1, is_distance=TRUE)
plot(tsne_out$Y,asp=1) # Plot the result
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(Word,tsne_out1) )%>%
left_join(dfm_count)%>%filter(Term_frequency<7000 & Term_frequency>500)
ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
geom_text_repel(aes(label=Word, size=Term_frequency),color="blue4", alpha=.5,max.overlaps = 50)
ggsave("tsne.jpeg", width = 28, height = 20, units = "cm")
foo<-UD%>%
group_by(doc_id)%>%
count(doc_id, lemma, sort=TRUE)
total_words <- foo %>%
group_by(doc_id) %>%
summarize(total = sum(n))
Avis_words <- left_join(foo, total_words) %>%
mutate(Term_frequency=n/total)%>%
group_by(doc_id)%>%
summarise(word=lemma,rank = row_number(),
n=n,
Term_frequency=Term_frequency)
Avis_words
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
#from session 1
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(syuzhet)  #analyse du sentimeent
# new for session 2
library(FactoMineR)
library(factoextra)
library(igraph)
library(ggwordcloud)
library(ggrepel)
library(Rtsne)
library(tidytext)
theme_set(theme_minimal())
t1=Sys.time()
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf <1 & tf_idf> .3) %>% #filtre sur le tfidf
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%select(-doc_id)
library(Rfast)
dist<-foo %>%
Dist(method="canberra2")
dist<-foo %>%
Dist(method="canberra1")
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf <1 & tf_idf> .3) %>% #filtre sur le tfidf
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)
dist<-foo %>%
dist(method="canberra")
#library(Rtsne) # Load package
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(dist,
initial_dims = 10,
perplexity = 20,
theta=.5,
num_threads=4, verbose=1, is_distance=TRUE)
dist<- as.matrix(dist)
View(dist)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf <1 & tf_idf> .3) %>% #filtre sur le tfidf
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf <1 & tf_idf> .3) %>% #filtre sur le tfidf
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>% select(-doc_id)
View(foo)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf <1 & tf_idf> .3) %>% #filtre sur le tfidf
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)%>%
replace(is.na(.), 0)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)%>%
replace(is.na(.), 0)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf>.1)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf>.1)%>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)%>%
replace(is.na(.), 0)
View(Avis_tf_idf)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf>.1)%>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)%>%
replace(is.na(.), 0)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf>.2)%>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id)%>%
replace(is.na(.), 0)
View(foo)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf>.2)%>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
select(-doc_id) %>%
replace(is.na(.), 0)
foo<-Avis_tf_idf %>%
select(doc_id, word, tf_idf) %>%
filter(tf_idf>.2)%>%
pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
dplyr::select(- doc_id) %>%
replace(is.na(.), 0)
View(foo)
dist<-t(foo) %>%
dist(method="canberra")
