select(je,vous, il, ils, pronomimp)
foo<-cbind(df,test)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(je=mean(je, na.rm=TRUE),
vous= mean(vous, na.rm=TRUE),
il_s=mean(il+ils,na.rm=TRUE),
pronomimp=mean(pronomimp,na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
my_text <- df$description
method <- "custom"
custom_lexicon <- data.frame(word=c("impot", "impôt","impots", "impôts", "taxe","taxes", "fisc", "fiscal", "fiscales", " fiscaux", "fiscalité", "redevance"),
value=c(1,1,1,1,1,1,1,1,1,1,1,1))
custom_distrib <- get_sentiment(my_text, method = method, lexicon = custom_lexicon)
custom_distrib<-as.data.frame(custom_distrib)
ggplot(custom_distrib,aes(x=custom_distrib))+geom_histogram()+scale_y_log10()
foo<-cbind(df,custom_distrib)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(custom_distrib=mean(custom_distrib, na.rm=TRUE))
ggplot(foo1,aes(x=date, y=custom_distrib))+
geom_line(size=1.2,stat="identity")+
labs(title = "Fisc", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#df$text<-str_replace(df$text, "\\w+", "J ") # trouver la solution!!!! pour le '
corpus<-corpus(df,id_field = "id",text_field = "description")
foo<-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)%>%
tokens_remove(stopwords("french"))
head(foo,5)
foo1 <-unlist_tokens(foo)
dim(foo1)
foo2<-foo1 %>%
group_by(token)%>%
summarise(n=n())%>%
mutate(rank=rank(desc(n)))
dim(foo2)
ggplot(foo2, aes(x=rank,y=n))+
geom_point(alpha=.2)+geom_smooth(method=lm)+
scale_x_log10()+
scale_y_log10()+
labs(title = "Zipf like")
#with cleaning
dfmat1 <- dfm(foo,
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat1, max_words = 50)
dfmat2 <- dfm(corpus_subset(corpus, intitule == "CAF"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 150)
t2=Sys.time()
t<- t2-t1
print(t)
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library("quanteda.dictionaries")
library(syuzhet)             #analyse du sentimeent
library(lubridate)
theme_set(theme_minimal())
t1=Sys.time()
#read the file and sample to reduce computation ( look at the end)
df <- read_csv("data_original_12062022_normalise_sentiment_transformers_lemna_words_adj.csv")%>%
select (id, date_ecrit, titre, description, ressenti, pays,intitule, canaux_typologie_1, transformer_sentiment_score) %>% sample_n(1000)
head(df, 5)
g1<-ggplot(df,aes(x=transformer_sentiment_score))+
geom_density(fill="pink", alpha=.5)
g2<-ggplot(df, aes(x=transformer_sentiment_score)) +
stat_ecdf(geom = "step",pad = FALSE)+
xlim(0,1)
#cowplot
plot_grid(
g1, g2,
labels = "AUTO"
)
#score et ressenti
g2<-ggplot(df, aes(y=transformer_sentiment_score, x=ressenti)) +
geom_violin(fill="pink", alpha=.5)+
ylim(0,1)+
geom_smooth()+
labs(x=NULL, y = "Score transformer")
g2
## multiple choice question recoding
typo<-as.data.frame(str_split_fixed(df$canaux_typologie_1, ",", 10))%>%
mutate(across(where(is.character), str_trim)) %>%
cbind(df$id) %>%
rename(id=11) %>%
pivot_longer(-id, names_to = "valeur", values_to="variable") %>%
filter(variable!="") %>%
group_by(id, variable) %>%
summarise(valeur=1) %>%
pivot_wider(id,names_from="variable", values_from="valeur")%>%
replace(is.na(.), 0)
#count the number of row
n_c<-nrow(typo)
n_c
foo<-typo %>%
pivot_longer(-id, names_to = "variable", values_to="value") %>%
group_by(variable)%>%
summarise(Penetration=mean(value))
ggplot(foo, aes(x=reorder(variable, Penetration),y=Penetration))+
geom_bar(stat="identity",fill="firebrick")+
coord_flip()+
labs(x=NULL, y="taux de pénétration")
#typo
foo<-typo%>%
select(-id)
#service
t<-as.data.frame(table(df$intitule)) %>%
filter(Freq>50)
ggplot(t, aes(x=reorder(Var1, Freq),y=Freq))+
geom_bar(stat="identity",fill="firebrick")+
coord_flip()+scale_y_log10()
foo<-df%>%mutate(n=1)%>%
group_by(intitule)%>%
summarise(score=mean(transformer_sentiment_score,na.rm=TRUE),n=sum(n)) %>%
filter(n>50)
ggplot(foo, aes(x=reorder(intitule, n), y=score ))+  geom_bar(stat="identity",fill="firebrick")+
coord_flip()
df$n_words<-str_count(df$description)
ggplot(df,aes(x=n_words))+
geom_density()+scale_x_log10()
foo<-df %>%
group_by(date_ecrit)%>%
summarise(n=n(),
size_t=mean(n_words))
ggplot(foo, aes(x=date_ecrit, y=n))+
geom_point()+
scale_y_log10()+
geom_smooth()
ggplot(foo, aes(x=date_ecrit, y=size_t))+
geom_point()+
scale_y_log10()+
geom_smooth()
ggplot(foo, aes(x=n, y=size_t))+
geom_point()+
scale_x_log10()+  scale_y_log10()+
geom_smooth()
#la fonction de calcul de lisibilité
readability<-textstat_readability(df$description,
measure = c("Flesch",
"meanSentenceLength",
"meanWordSyllables"))
foo<-cbind(df[,2],readability[,2:4])
foo$date<-as.POSIXct(foo$date)
foo1<-foo %>%
dplyr::mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
select(-Month, -date_ecrit,-Year)%>%
group_by(date) %>%
summarise(Flesch=mean(Flesch, na.rm=TRUE),
SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
WordSyllables= mean(meanWordSyllables, na.rm=TRUE))
foo2<-foo1 %>%
pivot_longer(-date,names_to="Variable", values_to="Score")%>%
drop_na()
ggplot(foo2,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Experience readability", x=NULL, y=NULL)
lexdiv<-tokens(df$description)%>%
textstat_lexdiv(df$text, measure = c("CTTR", "Maas"),  log.base = 10,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE)
foo<-cbind(df,lexdiv[,2:5])
foo1<-foo %>% mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(CTTR=mean(CTTR, na.rm=TRUE),
Maas= mean(Maas, na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Lexical diversity", x=NULL, y=NULL)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(CTTR=mean(CTTR, na.rm=TRUE),
Maas= mean(Maas, na.rm=TRUE))
cor(foo1$CTTR, foo1$Maas)
ggplot(foo1,aes(x=CTTR, y=Maas))+
geom_point(size=1.2, aes(color=date), stat="identity")+
labs(title = "Lexical diversity", x=NULL, y=NULL)+geom_smooth(method="lm")
#library(syuzhet) analyse du sentimeent
#paramétres
method <- "nrc"
lang <- "french"
phrase<-as.character(paste0(df$titre,". ",df$description))
#extraction
emotions <- get_nrc_sentiment(phrase,language = "french")
emotion<-emotions[,1:8]
polarity<-subset(emotions,select=c(positive, negative))
foo<-cbind(df,polarity)%>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))
#mean per
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
mutate(positive=positive/n_words,
negative=negative/n_words)%>%
group_by(date) %>%
summarise(positive=mean(positive, na.rm=TRUE),
negative= -mean(negative, na.rm=TRUE),
valence=positive+negative,
expressivity=positive-negative) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#library("quanteda.dictionaries")
dict_liwc_french <- dictionary(file = "FrenchLIWCDictionary.dic",
format = "LIWC")
test<-liwcalike(df$description,dictionary = dict_liwc_french) %>%
select(je,vous, il, ils, pronomimp)
foo<-cbind(df,test)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(je=mean(je, na.rm=TRUE),
vous= mean(vous, na.rm=TRUE),
il_s=mean(il+ils,na.rm=TRUE),
pronomimp=mean(pronomimp,na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
my_text <- df$description
method <- "custom"
custom_lexicon <- data.frame(word=c("impot", "impôt","impots", "impôts", "taxe","taxes", "fisc", "fiscal", "fiscales", " fiscaux", "fiscalité", "redevance"),
value=c(1,1,1,1,1,1,1,1,1,1,1,1))
custom_distrib <- get_sentiment(my_text, method = method, lexicon = custom_lexicon)
custom_distrib<-as.data.frame(custom_distrib)
ggplot(custom_distrib,aes(x=custom_distrib))+geom_histogram()+scale_y_log10()
foo<-cbind(df,custom_distrib)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(custom_distrib=mean(custom_distrib, na.rm=TRUE))
ggplot(foo1,aes(x=date, y=custom_distrib))+
geom_line(size=1.2,stat="identity")+
labs(title = "Fisc", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#df$text<-str_replace(df$text, "\\w+", "J ") # trouver la solution!!!! pour le '
corpus<-corpus(df,id_field = "id",text_field = "description")
foo<-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)%>%
tokens_remove(stopwords("french"))
head(foo,5)
foo1 <-unlist_tokens(foo)
dim(foo1)
foo2<-foo1 %>%
group_by(token)%>%
summarise(n=n())%>%
mutate(rank=rank(desc(n)))
dim(foo2)
ggplot(foo2, aes(x=rank,y=n))+
geom_point(alpha=.2)+geom_smooth(method=lm)+
scale_x_log10()+
scale_y_log10()+
labs(title = "Zipf like")
#with cleaning
dfmat1 <- dfm(foo,
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat1, max_words = 50)
dfmat2 <- dfm(corpus_subset(corpus, intitule == "CAF"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 150)
t2=Sys.time()
t<- t2-t1
print(t)
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library("quanteda.dictionaries")
library(syuzhet)             #analyse du sentimeent
library(lubridate)
theme_set(theme_minimal())
t1=Sys.time()
#read the file and sample to reduce computation ( look at the end)
df <- read_csv("data_original_12062022_normalise_sentiment_transformers_lemna_words_adj.csv")%>%
select (id, date_ecrit, titre, description, ressenti, pays,intitule, canaux_typologie_1, transformer_sentiment_score)
#%>% sample_n(1000)
head(df, 5)
g1<-ggplot(df,aes(x=transformer_sentiment_score))+
geom_density(fill="pink", alpha=.5)
g2<-ggplot(df, aes(x=transformer_sentiment_score)) +
stat_ecdf(geom = "step",pad = FALSE)+
xlim(0,1)
#cowplot
plot_grid(
g1, g2,
labels = "AUTO"
)
#score et ressenti
g2<-ggplot(df, aes(y=transformer_sentiment_score, x=ressenti)) +
geom_violin(fill="pink", alpha=.5)+
ylim(0,1)+
geom_smooth()+
labs(x=NULL, y = "Score transformer")
g2
## multiple choice question recoding
typo<-as.data.frame(str_split_fixed(df$canaux_typologie_1, ",", 10))%>%
mutate(across(where(is.character), str_trim)) %>%
cbind(df$id) %>%
rename(id=11) %>%
pivot_longer(-id, names_to = "valeur", values_to="variable") %>%
filter(variable!="") %>%
group_by(id, variable) %>%
summarise(valeur=1) %>%
pivot_wider(id,names_from="variable", values_from="valeur")%>%
replace(is.na(.), 0)
#count the number of row
n_c<-nrow(typo)
n_c
foo<-typo %>%
pivot_longer(-id, names_to = "variable", values_to="value") %>%
group_by(variable)%>%
summarise(Penetration=mean(value))
ggplot(foo, aes(x=reorder(variable, Penetration),y=Penetration))+
geom_bar(stat="identity",fill="firebrick")+
coord_flip()+
labs(x=NULL, y="taux de pénétration")
#typo
foo<-typo%>%
select(-id)
#service
t<-as.data.frame(table(df$intitule)) %>%
filter(Freq>50)
ggplot(t, aes(x=reorder(Var1, Freq),y=Freq))+
geom_bar(stat="identity",fill="firebrick")+
coord_flip()+scale_y_log10()
foo<-df%>%mutate(n=1)%>%
group_by(intitule)%>%
summarise(score=mean(transformer_sentiment_score,na.rm=TRUE),n=sum(n)) %>%
filter(n>50)
ggplot(foo, aes(x=reorder(intitule, n), y=score ))+  geom_bar(stat="identity",fill="firebrick")+
coord_flip()
df$n_words<-str_count(df$description)
ggplot(df,aes(x=n_words))+
geom_density()+scale_x_log10()
foo<-df %>%
group_by(date_ecrit)%>%
summarise(n=n(),
size_t=mean(n_words))
ggplot(foo, aes(x=date_ecrit, y=n))+
geom_point()+
scale_y_log10()+
geom_smooth()
ggplot(foo, aes(x=date_ecrit, y=size_t))+
geom_point()+
scale_y_log10()+
geom_smooth()
ggplot(foo, aes(x=n, y=size_t))+
geom_point()+
scale_x_log10()+  scale_y_log10()+
geom_smooth()
#la fonction de calcul de lisibilité
readability<-textstat_readability(df$description,
measure = c("Flesch",
"meanSentenceLength",
"meanWordSyllables"))
foo<-cbind(df[,2],readability[,2:4])
foo$date<-as.POSIXct(foo$date)
foo1<-foo %>%
dplyr::mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
select(-Month, -date_ecrit,-Year)%>%
group_by(date) %>%
summarise(Flesch=mean(Flesch, na.rm=TRUE),
SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
WordSyllables= mean(meanWordSyllables, na.rm=TRUE))
foo2<-foo1 %>%
pivot_longer(-date,names_to="Variable", values_to="Score")%>%
drop_na()
ggplot(foo2,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Experience readability", x=NULL, y=NULL)
lexdiv<-tokens(df$description)%>%
textstat_lexdiv(df$text, measure = c("CTTR", "Maas"),  log.base = 10,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE)
foo<-cbind(df,lexdiv[,2:5])
foo1<-foo %>% mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(CTTR=mean(CTTR, na.rm=TRUE),
Maas= mean(Maas, na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Lexical diversity", x=NULL, y=NULL)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(CTTR=mean(CTTR, na.rm=TRUE),
Maas= mean(Maas, na.rm=TRUE))
cor(foo1$CTTR, foo1$Maas)
ggplot(foo1,aes(x=CTTR, y=Maas))+
geom_point(size=1.2, aes(color=date), stat="identity")+
labs(title = "Lexical diversity", x=NULL, y=NULL)+geom_smooth(method="lm")
#library(syuzhet) analyse du sentimeent
#paramétres
method <- "nrc"
lang <- "french"
phrase<-as.character(paste0(df$titre,". ",df$description))
#extraction
emotions <- get_nrc_sentiment(phrase,language = "french")
emotion<-emotions[,1:8]
polarity<-subset(emotions,select=c(positive, negative))
foo<-cbind(df,polarity)%>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))
#mean per
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
mutate(positive=positive/n_words,
negative=negative/n_words)%>%
group_by(date) %>%
summarise(positive=mean(positive, na.rm=TRUE),
negative= -mean(negative, na.rm=TRUE),
valence=positive+negative,
expressivity=positive-negative) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#library("quanteda.dictionaries")
dict_liwc_french <- dictionary(file = "FrenchLIWCDictionary.dic",
format = "LIWC")
test<-liwcalike(df$description,dictionary = dict_liwc_french) %>%
select(je,vous, il, ils, pronomimp)
foo<-cbind(df,test)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(je=mean(je, na.rm=TRUE),
vous= mean(vous, na.rm=TRUE),
il_s=mean(il+ils,na.rm=TRUE),
pronomimp=mean(pronomimp,na.rm=TRUE)) %>%
pivot_longer(-date,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=date, y=Score, group=Variable))+
geom_line(size=1.2, aes(color=Variable), stat="identity")+
labs(title = "Sentiment", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
my_text <- df$description
method <- "custom"
custom_lexicon <- data.frame(word=c("impot", "impôt","impots", "impôts", "taxe","taxes", "fisc", "fiscal", "fiscales", " fiscaux", "fiscalité", "redevance"),
value=c(1,1,1,1,1,1,1,1,1,1,1,1))
custom_distrib <- get_sentiment(my_text, method = method, lexicon = custom_lexicon)
custom_distrib<-as.data.frame(custom_distrib)
ggplot(custom_distrib,aes(x=custom_distrib))+geom_histogram()+scale_y_log10()
foo<-cbind(df,custom_distrib)
foo1<-foo %>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
group_by(date) %>%
summarise(custom_distrib=mean(custom_distrib, na.rm=TRUE))
ggplot(foo1,aes(x=date, y=custom_distrib))+
geom_line(size=1.2,stat="identity")+
labs(title = "Fisc", x=NULL, y=NULL)+
scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
#df$text<-str_replace(df$text, "\\w+", "J ") # trouver la solution!!!! pour le '
corpus<-corpus(df,id_field = "id",text_field = "description")
foo<-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)%>%
tokens_remove(stopwords("french"))
head(foo,5)
foo1 <-unlist_tokens(foo)
dim(foo1)
foo2<-foo1 %>%
group_by(token)%>%
summarise(n=n())%>%
mutate(rank=rank(desc(n)))
dim(foo2)
ggplot(foo2, aes(x=rank,y=n))+
geom_point(alpha=.2)+geom_smooth(method=lm)+
scale_x_log10()+
scale_y_log10()+
labs(title = "Zipf like")
#with cleaning
dfmat1 <- dfm(foo,
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat1, max_words = 50)
dfmat2 <- dfm(corpus_subset(corpus, intitule == "CAF"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 150)
t2=Sys.time()
t<- t2-t1
print(t)
