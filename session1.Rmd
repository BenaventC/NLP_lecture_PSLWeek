---
title: "Session1 : text metric and word annotations"
author: "cb,jcrm,bc,oc"
date: "`r Sys.Date()`"
output: 
  html_document :
    toc: true
    toc_float: true
    toc_depth: 3
---

# Tools

Mainly [`Quanteda`](http://quanteda.io/) and [`udpipe`](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html)

```{r setup, include=TRUE,echo=TRUE,message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
library(tidyverse) # a swiss kniffe
library(udpipe) # syntaxis and lexicl annotations
library(flextable) #for table rendering
library(ggwordcloud) #for ploting
library(cowplot) #for ploting
library(quanteda) #a complete text mining package
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library("quanteda.dictionaries")
library(syuzhet)  #analyse du sentimeent
library(lubridate) #date processing
library(udpipe) #annotations
library(igraph) #network representations


theme_set(theme_minimal())

t1=Sys.time()
```

# Our Data

User Experience projet from DITP (Services public+)[ https://www.modernisation.gouv.fr/actualites/services-publics-plus-de-15-000-experiences-dusagers-partagees-sur-la-plateforme)]

Comment are relative to a life experience (renew a passeport) at a certain time, with a specific administration in a certain place.  

## Sentiment distribution (transformer method)

```{r 01}
#read the file and sample to reduce computation ( look at the end)
#we put the content of the csv file in a dataframe and select only certain columns
df <- read_csv("data_original_12062022_normalise_sentiment_transformers_lemna_words_adj.csv")%>%
  select (id, date_ecrit, titre, description, ressenti, pays,intitule, canaux_typologie_1, transformer_sentiment_score) 

# we treat the data
# the mutate function enables us to create or modify a column 
# here, we manipulate data for our future analysis
df<-df %>% 
  mutate(description=str_replace_all(description,"\\.(?=[A-Za-z])", ". "), #space after dot
         description=str_replace(description,"_", " "), #remove underscore
         description=str_replace(description,"(\u20AC)", "euro") )#change euro in euro


#%>% sample_n(1000)

head(df, 5) #we display the first 5 rows of the data frame
#we plot the transformer sentiment distribution, as a density and as a cumulative distribution 
g1<-ggplot(df,aes(x=transformer_sentiment_score))+
  geom_density(fill="pink", alpha=.5)

g2<-ggplot(df, aes(x=transformer_sentiment_score)) +
  stat_ecdf(geom = "step",pad = FALSE)+
  xlim(0,1)

#cowplot
#The plot_grid() function provides a simple interface for arranging plots into a grid and adding labels to them.
#https://wilkelab.org/cowplot/articles/plot_grid.html
plot_grid(
  g1, g2,
  labels = "AUTO"
  )
```


Compare with " smiles"

```{r 02}
#score et ressenti

g2<-ggplot(df, aes(y=transformer_sentiment_score, x=ressenti)) +
  geom_violin(fill="pink", alpha=.5)+
  ylim(0,1)+
  geom_smooth()+
  labs(x=NULL, y = "Score transformer")

g2
```

## channels involved
Les canaux d'interaction concernés par l'expérience. 

```{r 03}
## multiple choice question recoding
typo<-as.data.frame(str_split_fixed(df$canaux_typologie_1, ",", 10))%>%
  mutate(across(where(is.character), str_trim)) %>% 
  cbind(df$id) %>%
  rename(id=11) %>%
           pivot_longer(-id, names_to = "valeur", values_to="variable") %>%
  filter(variable!="") %>% 
  group_by(id, variable) %>% 
  summarise(valeur=1) %>%
  pivot_wider(id,names_from="variable", values_from="valeur")%>%
  replace(is.na(.), 0)

#count the number of row
n_c<-nrow(typo)
n_c
foo<-typo %>% 
  pivot_longer(-id, names_to = "variable", values_to="value") %>% 
  group_by(variable)%>%
  summarise(Penetration=mean(value)) 

ggplot(foo, aes(x=reorder(variable, Penetration),y=Penetration))+
  geom_bar(stat="identity",fill="firebrick")+
  coord_flip()+
  labs(x=NULL, y="taux de pénétration")
```

## Administrations involved

```{r 04}
#typo
foo<-typo%>%
  select(-id)

#service
t<-as.data.frame(table(df$intitule)) %>%
  filter(Freq>50)

ggplot(t, aes(x=reorder(Var1, Freq),y=Freq))+
  geom_bar(stat="identity",fill="firebrick")+
  coord_flip()+scale_y_log10()
```

Sentiment score comparison

```{r 05}
foo<-df%>%mutate(n=1)%>%
  group_by(intitule)%>%
  summarise(score=mean(transformer_sentiment_score,na.rm=TRUE),n=sum(n)) %>% 
  filter(n>50)

ggplot(foo, aes(x=reorder(intitule, n), y=score ))+  geom_bar(stat="identity",fill="firebrick")+
  coord_flip()

```

# Text level analysis 

(stylometry)

## counting

the number of words per review.

```{r 06}
df$n_words<-str_count(df$description)
ggplot(df,aes(x=n_words))+
  geom_density()+scale_x_log10()
```

Evolution of the number of reviews, size of reviews, and correlation between.

```{r 07}
foo<-df %>%
  group_by(date_ecrit)%>%
  summarise(n=n(),
            size_t=mean(n_words))

ggplot(foo, aes(x=date_ecrit, y=n))+
  geom_point()+
  scale_y_log10()+
  geom_smooth()

ggplot(foo, aes(x=date_ecrit, y=size_t))+
  geom_point()+
  scale_y_log10()+
  geom_smooth()

ggplot(foo, aes(x=n, y=size_t))+
  geom_point()+
  scale_x_log10()+  scale_y_log10()+
  geom_smooth()
```

## Readability

On agrège sur le mois, avec des fonctions `lubridate`


```{r 08, warning=TRUE}
#la fonction de calcul de lisibilité
readability<-textstat_readability(df$description, 
                                  measure = c("Flesch",
                                              "meanSentenceLength",
                                              "meanWordSyllables")) 

foo<-cbind(df[,2],readability[,2:4])
foo$date<-as.POSIXct(foo$date)

foo1<-foo %>% 
  dplyr::mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
  select(-Month, -date_ecrit,-Year)%>%
  group_by(date) %>%
  summarise(Flesch=mean(Flesch, na.rm=TRUE), 
            SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
            WordSyllables= mean(meanWordSyllables, na.rm=TRUE))


foo2<-foo1 %>%
  pivot_longer(-date,names_to="Variable", values_to="Score")%>%
  drop_na()

ggplot(foo2,aes(x=date, y=Score, group=Variable))+
  geom_line(size=1.2, aes(color=Variable), stat="identity")+
  facet_wrap(vars(Variable), scale="free", ncol=1)+
  labs(title = "Experience readability", x=NULL, y=NULL)
```

## Lexical diversity 

```{r 09}
lexdiv<-tokens(df$description)%>%
  textstat_lexdiv(df$text, measure = c("CTTR", "Maas"),  log.base = 10,
                  remove_numbers = TRUE,  
                  remove_punct = TRUE,  
                  remove_symbols = TRUE,
                  remove_hyphens = TRUE) 

foo<-cbind(df,lexdiv[,2:5])
foo1<-foo %>% mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
  group_by(date) %>%
  summarise(CTTR=mean(CTTR, na.rm=TRUE), 
            Maas= mean(Maas, na.rm=TRUE)) %>%
  pivot_longer(-date,names_to="Variable", values_to="Score")

ggplot(foo1,aes(x=date, y=Score, group=Variable))+
  geom_line(size=1.2, aes(color=Variable), stat="identity")+
  facet_wrap(vars(Variable), scale="free", ncol=1)+
  labs(title = "Lexical diversity", x=NULL, y=NULL)

foo1<-foo %>% 
  mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
  group_by(date) %>%
  summarise(CTTR=mean(CTTR, na.rm=TRUE), 
            Maas= mean(Maas, na.rm=TRUE))

cor(foo1$CTTR, foo1$Maas)
ggplot(foo1,aes(x=CTTR, y=Maas))+
  geom_point(size=1.2, aes(color=date), stat="identity")+
  labs(title = "Lexical diversity", x=NULL, y=NULL)+geom_smooth(method="lm")

```

## Sentiment analysis


a very simple function.


```{r 10, eval=TRUE}
#library(syuzhet) analyse du sentimeent

#paramétres
method <- "nrc"
lang <- "french"
phrase<-as.character(paste0(df$titre,". ",df$description))

#extraction
emotions <- get_nrc_sentiment(phrase,language = "french")

```

Now we plot the evolution

```{r 11}

emotion<-emotions[,1:8]
polarity<-subset(emotions,select=c(positive, negative))
foo<-cbind(df,polarity)%>%
mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))

#mean per 
foo1<-foo %>% 
  mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
  mutate(positive=positive/n_words, 
         negative=negative/n_words)%>%
  group_by(date) %>%
  summarise(positive=mean(positive, na.rm=TRUE), 
            negative= -mean(negative, na.rm=TRUE),
            valence=positive+negative,
            expressivity=positive-negative) %>%
  pivot_longer(-date,names_to="Variable", values_to="Score")

ggplot(foo1,aes(x=date, y=Score, group=Variable))+
  geom_line(size=1.2, aes(color=Variable), stat="identity")+
  labs(title = "Sentiment", x=NULL, y=NULL)+
  scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))
```

## Dictionnary methods : LIWC

```{r 12}

#library("quanteda.dictionaries")
dict_liwc_french <- dictionary(file = "FrenchLIWCDictionary.dic",
                             format = "LIWC")
test<-liwcalike(df$description,dictionary = dict_liwc_french) %>%
  select(je,vous, il, ils, pronomimp)
foo<-cbind(df,test)

foo1<-foo %>% 
  mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
  group_by(date) %>%
  summarise(je=mean(je, na.rm=TRUE), 
            vous= mean(vous, na.rm=TRUE),
            il_s=mean(il+ils,na.rm=TRUE),
            pronomimp=mean(pronomimp,na.rm=TRUE)) %>%
  pivot_longer(-date,names_to="Variable", values_to="Score")

ggplot(foo1,aes(x=date, y=Score, group=Variable))+
  geom_line(size=1.2, aes(color=Variable), stat="identity")+
  labs(title = "Sentiment", x=NULL, y=NULL)+
  scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))



```

## Own-made dictionnary

Just building list of words and given them a weight.

```{r 14}

my_text <- df$description
method <- "custom"
custom_lexicon <- data.frame(word=c("impot", "impôt","impots", "impôts", "taxe","taxes", "fisc", "fiscal", "fiscales", " fiscaux", "fiscalité", "redevance"),
                             value=c(1,1,1,1,1,1,1,1,1,1,1,1))
custom_distrib <- get_sentiment(my_text, method = method, lexicon = custom_lexicon)

custom_distrib<-as.data.frame(custom_distrib)
ggplot(custom_distrib,aes(x=custom_distrib))+geom_histogram()+scale_y_log10()

foo<-cbind(df,custom_distrib)

foo1<-foo %>% 
  mutate(Year=year(date_ecrit), Month=month(date_ecrit), date=my(paste(Month, Year)))%>%
  group_by(date) %>%
  summarise(custom_distrib=mean(custom_distrib, na.rm=TRUE))

ggplot(foo1,aes(x=date, y=custom_distrib))+
  geom_line(size=1.2,stat="identity")+
  labs(title = "Fisc", x=NULL, y=NULL)+
  scale_colour_manual(values=c("Orange"," Red", "Darkgreen","Grey"))


```


# Word level analysis

Annotation are apply at the level of words : lexical (stem, lemma, wordnet) and syntaxis annotations ( PS, syntaxis dependences).


## Tokens

The very first step. Usefull to set the vocabulary

```{r 015}
#df$text<-str_replace(df$text, "\\w+", "J ") # trouver la solution!!!! pour le '

corpus<-corpus(df,id_field = "id",text_field = "description")
foo<-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)%>%
  tokens_remove(stopwords("french"))

head(foo,5)

foo1 <-unlist_tokens(foo) 
dim(foo1)
foo2<-foo1 %>% 
  group_by(token)%>%
  summarise(n=n())%>%
              mutate(rank=rank(desc(n)))
                     
dim(foo2)

ggplot(foo2, aes(x=rank,y=n))+
  geom_point(alpha=.2)+geom_smooth(method=lm)+
  scale_x_log10()+
  scale_y_log10()+
  labs(title = "Zipf like")

#with cleaning

dfmat1 <- dfm(foo,
              remove = stopwords("french"), remove_punct = TRUE) %>%
   dfm_trim(min_termfreq = 3)

textplot_wordcloud(dfmat1, max_words = 50)


dfmat2 <- dfm(corpus_subset(corpus, intitule == "CAF"),
              remove = stopwords("french"), remove_punct = TRUE) %>%
   dfm_trim(min_termfreq = 3)

textplot_wordcloud(dfmat2, max_words = 150)

```


## collocation

looking for expressions


```{r 16}

#on sélectionne les mots commençant par une majuscule
toks_cap <- tokens_select(foo, 
                               pattern = "^[A-Z]",
                               valuetype = "regex",
                               case_insensitive = FALSE, 
                               padding = TRUE)

#on cherche les collocations
tstat_col_cap <- textstat_collocations(toks_cap, min_count = 3, tolower = FALSE)
#head(as.data.frame(tstat_col_cap),15)


toks_comp <- tokens_compound(foo, pattern = tstat_col_cap[tstat_col_cap$lamba > 10], 
                             case_insensitive = FALSE)

head(toks_comp)

```

## Lemmas and POS

```{r 17, eval=FALSE}

library(udpipe)
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")

UD <- udpipe_annotate(udmodel_french, x=df$description, trace =1000)
UD <- as.data.frame(UD)
saveRDS(UD, "UD.rds")
```

Let examine the content : lemma per POS.

```{r 18}
UD<-readRDS("UD.rds")
foo<-UD%>%
  group_by(upos)%>%
  summarise(n=n())%>%
  ggplot(aes(x=reorder(upos,n), y=n))+
  geom_bar(stat = "identity")+coord_flip()
foo
foo<-UD %>%
  filter(upos=="NOUN") %>%
  group_by(lemma)%>%
  summarise(n=n()) %>%
  filter(n>1000)%>%
  ggplot(aes(x=reorder(lemma,n), y=n))+
  geom_point(size=1.5, fill="blue3")+coord_flip()

foo

foo<-UD %>%
  filter(upos=="ADJ" |upos=="VERB") %>%
  group_by(lemma, upos)%>%
  summarise(n=n()) %>%
  filter(n>300)%>%
  ggplot(aes(label = lemma, size = log(n), group=upos)) +
  geom_text_wordcloud(aes(color=upos)) +
  theme_minimal()+
  facet_wrap(vars(upos))

foo
```


## syntaxic dependance

A few `dplyr gymnastics', and thats already the session 2 . 

[We follow](https://rpubs.com/pjmurphy/317838) for idea of code.

for dataviz `igraph` is the tool.


```{r 19, fig.width=12}
foo1<-UD %>%
  mutate(id=paste0(doc_id,paragraph_id,sentence_id,token_id))%>%
  select(id, lemma)%>%
  rename(noun=lemma)

foo<-UD %>%
  filter(dep_rel=="amod")%>%
  mutate(id=paste0(doc_id,paragraph_id,sentence_id,head_token_id))%>%
  left_join(foo1)%>%
  select(id, lemma, noun)%>%
  rename(adj=lemma)%>%
  group_by(noun,adj)%>%
  summarise(n=n())%>%
  filter(n>50)
#A  Correspondance Analysis solution?
#igraph approach belong to the netx lesson 
library(igraph)
g <- graph.data.frame(foo, directed=FALSE)
V(g)$type <- bipartite_mapping(g)$type  ## Add the "type" attribute
V(g)$label.color <- ifelse(V(g)$type, "salmon4", "blue2")
V(g)$fill <- ifelse(V(g)$type, "salmon4", "blue2")
V(g)$shape <-ifelse(V(g)$type, "circle", "square")
plot(g,vertex.label.cex = 0.8)
```

# Notes

Beware to computing time! Best to sample for testing. (come back to the beginning)

```{r 20}
t2=Sys.time()
t<- t2-t1
print(t)
```

We save the annotated dataset in a format *.rds for further usage. 

```{r 021}
df_work<-cbind(df,readability, lexdiv,emotions,test)
write_rds(df_work,"df_work.rds")
```

See you tomorrow and [go to session 2](https://benaventc.github.io/NLP_lecture_PSLWeek/session1.html)

Some exercises before, for training :

* compute the sentiment évolution for one administration of your choice.
* ...



