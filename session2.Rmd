---
title: "Session2 : Semantic spaces"
author: "cb,jcrm,bc,oc"
date: "`r Sys.Date()`"
output: 
  html_document :
    toc: true
    toc_float: true
    toc_depth: 3
---

# Tools

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
#from session 1
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(syuzhet)  #analyse du sentimeent

# new for session 2
library(FactoMineR)
library(factoextra)
library(igraph)
library(ggwordcloud)
library(ggrepel)

library(Rtsne)
library(tidytext)

theme_set(theme_minimal())

t1=Sys.time()

```

# Dfm & Co-occurence matrix

We start from the annotation task. First objective is to reconstitute tex with lemma. Because we are first interesting par the content, we will work mainly with noun, proper name and verbs. Things, people and action. We will add also adjectives as there are more attributives than qualifying. (fact versus jugement technics)


```{r 00}

#read the file and select tokens
UD<-readRDS("./Data/UD.rds")
df<-UD %>%
  select(doc_id, lemma, upos)%>%
  filter(upos=="NOUN"|upos=="PROPN"|upos=="VERB"|upos=="ADJ")%>%
  group_by(doc_id)%>%
  summarise(text=paste(lemma, collapse=" "))

head(df,5)

#add doc_var  variables 

```

Come back to `quanteda`, making a dfm and a cfm.

```{r 01}
corpus <- corpus(df$text) # docvars(corpus, field = "doc_id")
summary(corpus)

dfm<- corpus %>%
  tokens() %>% 
  tokens_remove(stopwords("french"))%>%
  dfm()

dfm<- dfm %>%
    dfm_trim(min_termfreq = 50, verbose = FALSE)  
#a rule of thumb frequency must be at least2 times the density of 2 for a Thousand

#wordcloud
set.seed(100)
textplot_wordcloud(dfm)

dfm_df<- as.data.frame(dfm) 

dim(dfm_df)
head(dfm,8)

#counting words
dfm_count<-dfm_df %>%
  pivot_longer(-doc_id,names_to = "word", values_to = "n")%>%
  group_by(word)%>% 
  summarise(Frequency=sum(n))
```

# Map the tokens

Issue is that with wordcloud position is meaningless, just random and paving..

Goal : represent similarity between words based on their co-occurences in a small dimension space.

## PCA approach

Come back to PCA ! ( we pass CA and MCA)

```{r 02}
# with library(FactoMineR)
foo<- dfm_df %>%
  column_to_rownames(var="doc_id")

#PCA
res.pca <- PCA(foo[,c(1:2905)], ncp=10,graph=FALSE) #could be pretty long
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 10))

#label selection factor
a=.2

foo1<-as.data.frame(res.pca$var$coord) %>% 
  rownames_to_column(var="label") %>%
  filter(Dim.1>a | Dim.1< -a | Dim.2>a | Dim.2< - a)

ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
  geom_text(aes(label=label),size=2)

foo1<-as.data.frame(res.pca$var$coord) %>% 
  rownames_to_column(var="label") %>%
  filter(Dim.3> a | Dim.3< -a | Dim.4> a | Dim.4< -a)

ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
  geom_text(aes(label=label),size=2)

#typo with kmnes

foo<-as.data.frame(res.pca$var$coord)
resKM <- kmeans(foo, 20, nstart = 25, trace=0)


foo<-as.data.frame(resKM$cluster) %>%
  rename(cluster=1)%>% 
  rownames_to_column(var="word") %>%
  left_join(dfm_count)
foo$cluster<-as.factor(foo$cluster)


set.seed(42) #for reproducibility

#library(ggwordcloud)
ggplot(foo, aes(label = word, size=Frequency, group = cluster)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 5) +
  facet_wrap(vars(cluster), ncol=4)+
  theme_minimal()+ 
  labs(title=NULL) #comment enlever la numérotation ?

ggsave("./Images/cluster.jpeg", width = 28, height = 20, units = "cm")

```

## Tsne Approach

A legacy of MDS ( multidimentional scaling)


tsne to project large number of object in a small dimension space as tradition MDS, but with a principle of density, that dilate space where point are dense, and contract it when is not. 

https://cran.r-project.org/web/packages/Rtsne/Rtsne.pdf

Distance criteria : Canberra distance. 

$$d_{ij}= \sum_{x = 1}^{k} \frac{x_{ik}-x_{jk}}{x_{ik}+x_{jk}}$$


The best alternative seems to be  UMAP (add the link)

first compute distance between words

```{r 03}
#we need to transpose the matrix
foo<-dfm_df %>%
  select(-doc_id)%>% 
  t()

word<-as.data.frame(rownames(foo))%>%
  rename(word=1)

#library(Rfast) à retester
#dist<-foo %>%dist(method="canberra") #ts computing intensive. 
saveRDS(dist, "./data/dist1.rds")
```

Model and 2D tsne representation 

```{r 04}
#library(Rtsne) # Load package
#readRDS(dist1.rds)
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(dist,
                  initial_dims = 50,
                  perplexity = 30,
                  theta=.5,
                  num_threads=4, 
                  verbose=1, 
                  is_distance=TRUE)

tsne_out1<-as.data.frame(tsne_out$Y)
tsne_out2<-cbind(word,tsne_out1)%>%
  left_join(dfm_count)%>%
  filter(Frequency<7000 & Frequency>300)

ggplot(tsne_out2, aes(x=V1, y=V2))+
  geom_text_repel(aes(label=word, 
                      size=ifelse(Term_frequency>200,log10(Term_frequency),.5),
                      alpha=log10(Term_frequency)),
                  color="black",
                  max.overlap=30)+
  theme(legend.position = "none")+
  labs(x=NULL, y=NULL)+  
  scale_size(range = c(.5, 3))

ggsave("./Images/tsne.jpeg", width = 28, height = 20, units = "cm")

```

## Semantic Networks

A quick and dirty `quanteda` function `textplot_network`

It is based on `igraph`. that will be the end of the lecture. 

```{r 05}
##coocurrence computing
tag_fcm <- fcm(dfm) 
head(tag_fcm)

#select top tags
toptag <- names(topfeatures(dfm, 300)) 

topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) #select 200 more frequent words

textplot_network(topgat_fcm, 
                 min_freq = 0.10, 
                 edge_alpha = 0.005, 
                 edge_size = 0.1,
                vertex_labelsize = 2.5)

ggsave("./Images/network.jpeg", width = 28, height = 20, units = "cm")

```

# Tfidf matrix

Raw frequencies could be misleading as frequent word are not always distinctive. The idea is to weigth term frequency with document frequency, which is the numbre of document in which you could observe the terms . 

$$TfIdf_{i}=Tf_{i}\ln(\frac{N}{n_{i}})$$ 
where N is the Total number of document and $n_{i}$ the number of document in which term i is present.

To change style of writing operation, we will use `tidytext`. The code [come from](https://www.tidytextmining.com/tfidf.html) 

```{r 06}
foo<-UD %>%
  filter(upos=="NOUN"|upos=="PROPN"|upos=="VERB"|upos=="ADJ")%>%
  group_by(doc_id)%>%
  count(doc_id, lemma, sort=TRUE)

total_words <- foo %>% 
  group_by(doc_id) %>% 
  summarize(total = sum(n))

Avis_words <- left_join(foo, total_words) %>%
  mutate(Term_frequency=n/total)%>%
  group_by(doc_id)%>%
  summarise(word=lemma,rank = row_number(),
            n=n,
            Term_frequency=Term_frequency)
Avis_words
```

Just test on two documents to get a flavour of the story.

Distribution and zipf like plot.

```{r 07}
Avis_words%>%
  filter(doc_id=="doc12441"|doc_id=="doc12912") %>% 
  ggplot(aes(x=Term_frequency, fill = doc_id)) +
  geom_histogram(show.legend = FALSE) +
#  xlim(NA, 0.0009) +
  facet_wrap(~doc_id, ncol = 2, scales = "free_y")


Avis_words%>%
  filter(doc_id=="doc12441"|doc_id=="doc12912") %>% 
  ggplot(aes(x=rank,Term_frequency, color = doc_id)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```

tfidf computing with tidytext function `bind_tf_idf`


```{r 08}
Avis_tf_idf <- Avis_words %>%
  bind_tf_idf(word, doc_id, n)

Avis_tf_idf %>%
   select(-rank,n,Term_frequency) %>% 
  filter(tf_idf<0.5)%>%
  arrange(desc(tf_idf))

Avis_tf_idf%>%
  filter(doc_id=="doc12441"|doc_id=="doc12912") %>% 
  group_by(doc_id) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = doc_id)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~doc_id, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

Avis_tf_idf%>%
  ggplot(aes(tf_idf))+
  geom_histogram(binwidth = 0.002)+
  xlim(0,1)


```

Reuse of tsne procedure. But first compute distance


```{r 09}
foo<-Avis_tf_idf %>% 
  left_join(dfm_count) %>%
  filter(Frequency>50)%>%
  select(doc_id, word, tf_idf) %>% 
  pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
 column_to_rownames(var="doc_id")

library(Rfast)
dist2<-t(foo) %>%
  Dist(method="canberra2")

saveRDS(dist2, "./data/dist2.rds")
```


```{r 09b}
dist2<-readRDS("./data/dist2.rds")

set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(dist,
                  initial_dims = 10,
                  perplexity = 30,
                  theta=.5,
                  num_threads=4, verbose=1, is_distance=TRUE)
plot(tsne_out$Y,asp=1) # Plot the result


tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1) )%>%
  left_join(dfm_count)%>%
  filter(Frequency<7000 & Frequency>200)

ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
  geom_text_repel(aes(label=word, size=log10(Frequency)),color="blue4", alpha=.5,max.overlaps = 50)

ggsave("tsne2.jpeg", width = 28, height = 20, units = "cm")
```

# Compare groups

Playing with kwic ( from NPS game)

# More about networks

a complete graph with igraph


# Notes

```{r 20}
t2=Sys.time()
t<- t2-t1
print(t)
```
