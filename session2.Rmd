---
title: "Session 2 : semantic space"
author: "cb,jcrm,bc,oc"
date: "`r Sys.Date()`"
output: 
  html_document :
    toc: true
    toc_float: true
    toc_depth: 3
bibliography: [PLSweek2022.bib]
---

# Tools

@blei_latent_2003
Hello, this is a test.
Goodbye, another test.


```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
#from session 1
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(syuzhet)  #analyse du sentimeent

# new for session 2
library(FactoMineR)
library(factoextra)
library(igraph)
library(ggwordcloud)
library(ggrepel)

library(Rtsne)
library(tidytext)
theme_set(theme_minimal())

t1=Sys.time()

```

# Dfm & Co-occurence matrix

We start from the annotation task. First objective is to reconstitute text with lemma. Because we are first interesting par the content, we will work mainly with noun, proper name and verbs. Things, people and action. We will add also adjectives as there are more attributives than qualifying. (fact versus judgement technics) @van_der_maaten_laurens_visualizing_2008


```{r 00}

#read the file and select tokens
UD<-readRDS("./Data/UD.rds")
df<-UD %>%
  select(doc_id, lemma, upos)%>%
  filter(upos=="NOUN"|upos=="PROPN"|upos=="VERB"|upos=="ADJ")%>%
  group_by(doc_id)%>%
  summarise(text=paste(lemma, collapse=" "))

head(df,5)

#add doc_var  variables 
```

Come back to `quanteda`, making a dfm and a cfm.

```{r 01}
corpus <- corpus(df$text) # docvars(corpus, field = "doc_id")
summary(corpus)

dfm<- corpus %>%
  tokens() %>% 
  tokens_remove(stopwords("french"))%>%
  dfm()

dfm<- dfm %>%
    dfm_trim(min_termfreq = 50, verbose = FALSE)  
#a rule of thumb frequency must be at least2 times the density of 2 for a Thousand

#wordcloud
set.seed(100)
textplot_wordcloud(dfm)

dfm_df<- as.data.frame(dfm) 

dim(dfm_df)
head(dfm,8)

#counting words
dfm_count<-dfm_df %>%
  pivot_longer(-doc_id,names_to = "word", values_to = "n")%>%
  group_by(word)%>% 
  summarise(Frequency=sum(n))
```

# Map the tokens

Issue is that with wordcloud position is meaningless, just random and paving..

Goal : represent similarity between words based on their co-occurences in a small dimension space.

## PCA approach

Come back to PCCA ! (we pass CA and MCA)

```{r 02}
# with library(FactoMineR)
foo<- dfm_df %>%
  column_to_rownames(var="doc_id")

#PCA
res.pca <- PCA(foo[,c(1:1658)], ncp=10,graph=FALSE) #could be pretty long
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 10))

#label selection factor
a=.2

foo1<-as.data.frame(res.pca$var$coord) %>% 
  rownames_to_column(var="label") %>%
  filter(Dim.1>a | Dim.1< -a | Dim.2>a | Dim.2< - a)

ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
  geom_text(aes(label=label),size=2)

foo1<-as.data.frame(res.pca$var$coord) %>% 
  rownames_to_column(var="label") %>%
  filter(Dim.3> a | Dim.3< -a | Dim.4> a | Dim.4< -a)

ggplot(foo1, aes(x=Dim.1, y=Dim.2))+
  geom_text(aes(label=label),size=2)

#typo with kmnes

foo<-as.data.frame(res.pca$var$coord)
resKM <- kmeans(foo, 20, nstart = 25, trace=0)


foo<-as.data.frame(resKM$cluster) %>%
  rename(cluster=1)%>% 
  rownames_to_column(var="word") %>%
  left_join(dfm_count)
foo$cluster<-as.factor(foo$cluster)


set.seed(42) #for reproducibility

#library(ggwordcloud)
ggplot(foo, aes(label = word, size=Frequency, group = cluster)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 5) +
  facet_wrap(vars(cluster), ncol=4)+
  theme_minimal()+ 
  labs(title=NULL) #comment enlever la numérotation ?

ggsave("./Images/cluster.jpeg", width = 28, height = 20, units = "cm")

```

## Tsne Approach

A legacy of MDS (Multidimensional scaling) due to Van der maaten. Source is 

T-sne project large number of objects in a small dimension space as traditional MDS, but with a principle of density, that dilate space where point are dense, and contract it when is not. 

https://cran.r-project.org/web/packages/Rtsne/Rtsne.pdf


The best alternative seems to be  [UMAP](https://cran.r-project.org/web/packages/umap/vignettes/umap.html).


```{r 03}
#we need to transpose the matrix
foo<-dfm_df %>%
  select(-doc_id)%>%t()

#just to keep words aside
word<-as.data.frame(rownames(foo))%>%
  rename(word=1)

```

Model and 2D tsne representation 

```{r 04}
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo,
                  initial_dims = 50,
                  perplexity = 50,
                  partial_pca=TRUE,
                  theta=.5,
                  num_threads=4, 
                  verbose=1)

                  
tsne_out1<-as.data.frame(tsne_out$Y)
tsne_out2<-cbind(word,tsne_out1)%>%
  left_join(dfm_count)%>%
  filter(Frequency<7000 & Frequency>150)

ggplot(tsne_out2, aes(x=V1, y=V2))+
  geom_text_repel(aes(label=word, 
                      size=log10(Frequency),
                      alpha=log10(Frequency)),
                  color="black",
                  max.overlap=Inf)+
  theme(legend.position = "none")+
  labs(x=NULL, y=NULL)+  
  scale_size(range = c(.1, 3.5))

ggsave("./Images/tsne.jpeg", width = 28, height = 20, units = "cm")

```

## Semantic Networks

A quick and dirty `quanteda` function `textplot_network`

It is based on `igraph`. that will be the end of the lecture. 

```{r 05}
##coocurrence computing
tag_fcm <- fcm(dfm) 
head(tag_fcm)

#select top tags
toptag <- names(topfeatures(dfm, 100)) 

topgat_fcm <- fcm_select(tag_fcm, pattern = toptag) #select 500 more frequent words

textplot_network(topgat_fcm, 
                 min_freq = 0.5, 
                 edge_alpha = 0.01, 
                 edge_size = 0.1,
                vertex_labelsize = 2.5)

ggsave("./Images/network.jpeg", width = 28, height = 20, units = "cm")

```

# Tfidf matrix

Raw frequencies could be misleading as frequent word are not always distinctive. The idea is to weigth term frequency with document frequency, which is the numbre of document in which you could observe the terms . 

$$TfIdf_{i}=Tf_{i}\ln(\frac{N}{n_{i}})$$ 
where N is the Total number of document and $n_{i}$ the number of document in which term i is present.

To change style of writing operation, we will use `tidytext`. The code [come from](https://www.tidytextmining.com/tfidf.html) 

```{r 06}
foo<-UD %>%
  filter(upos=="NOUN"|upos=="PROPN"|upos=="VERB"|upos=="ADJ")%>%
  group_by(doc_id)%>%
  count(doc_id, lemma, sort=TRUE)

total_words <- foo %>% 
  group_by(doc_id) %>% 
  summarize(total = sum(n))

Avis_words <- left_join(foo, total_words) %>%
  mutate(Term_frequency=n/total)%>%
  group_by(doc_id)%>%
  summarise(word=lemma,rank = row_number(),
            n=n,
            Term_frequency=Term_frequency)
Avis_words
```

Just test on two documents to get a flavour of the story.

Distribution and zipf like plot.

```{r 07}
Avis_words%>%
  filter(doc_id=="doc12441"|doc_id=="doc12912") %>% 
  ggplot(aes(x=Term_frequency, fill = doc_id)) +
  geom_histogram(show.legend = FALSE) +
#  xlim(NA, 0.0009) +
  facet_wrap(~doc_id, ncol = 2, scales = "free_y")


Avis_words%>%
  filter(doc_id=="doc12441"|doc_id=="doc12912") %>% 
  ggplot(aes(x=rank,Term_frequency, color = doc_id)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

```

tfidf computing with tidytext function `bind_tf_idf`


```{r 08}
Avis_tf_idf <- Avis_words %>%
  bind_tf_idf(word, doc_id, n)

Avis_tf_idf %>%
   select(-rank,n,Term_frequency) %>% 
  filter(tf_idf<0.5)%>%
  arrange(desc(tf_idf))

Avis_tf_idf%>%
  filter(doc_id=="doc12441"|doc_id=="doc12912") %>% 
  group_by(doc_id) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = doc_id)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~doc_id, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

Avis_tf_idf%>%
  ggplot(aes(tf_idf))+
  geom_histogram(binwidth = 0.002)+
  xlim(0,1)


```

Reuse of tsne procedure. But first compute distance


```{r 09}
foo<-Avis_tf_idf %>% 
  left_join(dfm_count) %>%
  filter(Frequency>50)%>%
  select(doc_id, word, tf_idf) %>% 
  pivot_wider(doc_id, names_from = "word", values_from = "tf_idf")%>%
 column_to_rownames(var="doc_id") %>%
  mutate(
    across(everything(), ~replace_na(.x, 0))
  )

#just to keep words aside

foo<-t(foo)
word<-as.data.frame(rownames(foo))%>%
  rename(word=1)

```

then apply the model

```{r 09b}

set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo,
                  initial_dims = 50,
                  perplexity = 50,
                  partial_pca=TRUE,
                  theta=.5,
                  num_threads=4, 
                  verbose=1,
                  check_duplicates = FALSE)
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1) )%>%
  left_join(dfm_count)%>%
  filter(Frequency<7000 & Frequency>100)

ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
  geom_text_repel(aes(label=word, 
                      size=log10(Frequency),
                      alpha=log10(Frequency)),
                  color="black",
                  max.overlap=Inf)+
  theme(legend.position = "none")+
  labs(x=NULL, y=NULL)+  
  scale_size(range = c(.1, 3))

ggsave("./Images/tsne2.jpeg", width = 28, height = 20, units = "cm")
```

# Compare groups

Playing with Keyness, a statistics fo find discriminant words across group of text. https://quanteda.io/reference/textstat_keyness.html

( from NPS game)

```{r 10}

# 1 corpus definition
df_work<-readRDS("./Data/df_work.rds")

corpus<-corpus(df_work,text_field ="description")

# 2 tokenisation (per group)

toks <- tokens(corpus, remove_punct = TRUE) %>% 
    tokens_remove(pattern = stopwords("fr"))%>%
   tokens_remove(pattern="très*") %>%
    tokens_group(groups = ressenti)

# 3 dfm building 

dfm <- dfm(toks) %>%   
  dfm_trim(min_termfreq = 40, verbose = FALSE)

# 4 afficher le wordcloud

textplot_wordcloud(dfm,comparison = TRUE, color = col)

```

Keyness for positive and negative. 

A clear answer ! 

```{r 11}

# Create a dfm per group
dfm <-toks %>%
    tokens_group(groups = ressenti) %>% 
  dfm()

# Calculate keyness and determine "Positif" as target group againts all other categories
result_keyness <- textstat_keyness(dfm, target = "Positif") %>% 
  filter (n_target>20)

# Plot estimated word keyness
g1<-textplot_keyness(result_keyness,   n = 20L, labelsize = 3,   show_legend = FALSE, 
                     show_reference = TRUE,   color = c("Darkgreen", "gray"))+
  labs(x=NULL)

# Calculate keyness and determine "Négatif" as target group againts all other categories
result_keyness <- textstat_keyness(dfm, target = "Négatif") %>% 
  filter (n_target>20)

# Plot estimated word keyness
g2<-textplot_keyness(result_keyness,   n = 20L, labelsize = 3,   show_legend = FALSE, 
                     show_reference = TRUE,   color = c("firebrick", "gray"))+
  labs(x=NULL)

plot_grid(
  g1, g2,
  labels = "AUTO"
  )

ggsave("./Images/Keyness.jpeg", width = 28, height = 20, units = "cm")

```


# More about networks

a complete graph with igraph


# Notes

Beware to computing time! Best to sample for testing. (come back to the beginning)

```{r 20}
t2=Sys.time()
t<- t2-t1
print(t)
```


See you to later and [go to session 3](https://benaventc.github.io/NLP_lecture_PSLWeek/session3.html)

Some exercises before, for training :

* Explore the adjectives world
* Compare administration (main) in term of qualificatives.

# References
