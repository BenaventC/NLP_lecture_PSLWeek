
@article{van_der_maaten_laurens_visualizing_2008,
	title = {Visualizing Data using t-{SNE}},
	pages = {2579--2605},
	journaltitle = {Journal of Machine learning},
	author = {{Van der Maaten, Laurens} and Hinton, Geoffrey},
	date = {2008},
}

@article{blei_latent_2003,
	title = {Latent Dirichlet Allocation},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944937},
	pages = {993--1022},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	date = {2003-03},
}

@article{kozlowski_geometry_nodate,
	title = {The Geometry of Culture: Analyzing Meaning through Word Embeddings},
	pages = {73},
	author = {Kozlowski, Austin C and Taddy, Matt and Evans, James A},
	langid = {english},
	file = {Kozlowski et al. - The Geometry of Culture Analyzing Meaning through.pdf:C\:\\Users\\33623\\Zotero\\storage\\9GI73VVT\\Kozlowski et al. - The Geometry of Culture Analyzing Meaning through.pdf:application/pdf},
}

@misc{hamilton_diachronic_2018,
	title = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
	url = {http://arxiv.org/abs/1605.09096},
	abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings ({PPMI}, {SVD}, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity—the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation—independent of frequency, words that are more polysemous have higher rates of semantic change.},
	number = {{arXiv}:1605.09096},
	publisher = {{arXiv}},
	author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
	urldate = {2022-07-11},
	date = {2018-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.09096 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Hamilton et al. - 2018 - Diachronic Word Embeddings Reveal Statistical Laws.pdf:C\:\\Users\\33623\\Zotero\\storage\\3N25AGFK\\Hamilton et al. - 2018 - Diachronic Word Embeddings Reveal Statistical Laws.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	number = {{arXiv}:1301.3781},
	publisher = {{arXiv}},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2022-07-19},
	date = {2013-09-06},
	eprinttype = {arxiv},
	eprint = {1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\33623\\Zotero\\storage\\B23WHTTA\\1301.html:text/html},
}

@collection{bondi_keyness_2010,
	location = {Amsterdam ; Philadelphia},
	title = {Keyness in texts},
	isbn = {978-90-272-2317-3 978-90-272-8766-3},
	series = {Studies in corpus linguistics},
	pagetotal = {251},
	number = {v. 41},
	publisher = {John Benjamins Pub. Co},
	editor = {Bondi, Marina and Scott, Mike},
	date = {2010},
	note = {{OCLC}: ocn647977619},
	keywords = {Semantics, Corpora (Linguistics), Discourse analysis, Phraseology},
}

@article{gennaro_emotion_2022,
	title = {Emotion and Reason in Political Language},
	volume = {132},
	issn = {0013-0133, 1468-0297},
	url = {https://academic.oup.com/ej/article/132/643/1037/6490125},
	doi = {10.1093/ej/ueab104},
	abstract = {Abstract
            This paper studies the use of emotion and reason in political discourse. Adopting computational-linguistics techniques to construct a validated text-based scale, we measure emotionality in six million speeches given in U.S. Congress over the years 1858–2014. Intuitively, emotionality spikes during times of war and is highest in speeches about patriotism. In the time series, emotionality was relatively low and stable in earlier years but increased significantly starting in the late 1970s. Across Congress members, emotionality is higher for Democrats, for women, for ethnic/religious minorities, for the opposition party and for members with ideologically extreme roll-call voting records.},
	pages = {1037--1059},
	number = {643},
	journaltitle = {The Economic Journal},
	author = {Gennaro, Gloria and Ash, Elliott},
	urldate = {2022-11-02},
	date = {2022-04-01},
	langid = {english},
}
