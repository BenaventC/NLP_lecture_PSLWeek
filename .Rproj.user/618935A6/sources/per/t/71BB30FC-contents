---
title: "Session 4 : Embeddings"
author: "cb,jcrm,bc,oc"
date: "`r Sys.Date()`"
output: 
  html_document :
    toc: true
    toc_float: true
    toc_depth: 3
bibliography: [PLSweek2022.bib]
---

# Tools

```{r 00, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, message=FALSE, warning=FALSE)
#from session 1 & 2
library(tidyverse)
library(udpipe)
library(flextable)
library(cowplot)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(syuzhet)  #analyse du sentimeent

# new for session 3
library(FactoMineR)
library(factoextra)
library(igraph)
library(ggwordcloud)
library(ggrepel)

library(Rtsne)
library(tidytext)

#new for session 4
library(word2vec)
library(ape)

theme_set(theme_minimal()+theme(plot.title = element_text(size=12)))

t1=Sys.time()

```

# Building embeddings

The idea came from information retrieval with the work of Sutton for example. But the great idea came from @mikolov_efficient_2013.


## A few theory

Main ideas : encode a word with vector of large dimension. As the meaning of word come from their cooccurent usage () within a certain window of words, vectors of words will be similar in the space. 

![Word as vector](./Images/wordvectors.jpg)

Two algorithms : 

* Skip-gram: works well with a small amount of the training data, represents well even rare words or phrases.
 * CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.

![Algorithms](./Images/cbow_skipgram.jpg)

## Application

we use here the [package word2vec](https://www.rdocumentation.org/packages/word2vec/versions/0.3.4), but some other solutions are offered with  `WordtoVec` ( berndt schmidt) or glove from `quanteda`

```{r 01}
UD<-readRDS("./data/UD.rds")

#on filtre adverbes adjectifs verb et non communs
updated_vocab <- UD %>%  
  filter(upos %in% c('NOUN', 'PROPN', "ADJ","VERB")) %>% 
  mutate(lemma=tolower(lemma))

updated_vocab2<- updated_vocab %>% 
  rename(word=lemma)%>%
  group_by(word)%>% 
  summarise(n=n())

#on reconstitue le texte filtré
text2<-updated_vocab %>%
 group_by(doc_id) %>%
 summarise(description = paste(lemma, collapse = " "))


#on vectorise
set.seed(123456789)
model <- word2vec(x = text2$description, 
                  type = "cbow", 
                  window = 7, 
                  dim = 200, 
                  iter = 100,
                  verbose=10,
                    threads = 4L
                  )
embedding <- as.matrix(model)

#test sur reviews

lookslike <- predict(model, c("impot"), type = "nearest", top_n = 20)
foo<-lookslike$impot
g1<-ggplot(foo, aes(x=reorder(term2,similarity),y=similarity))+
  geom_point(col="black",size=3)+
  coord_flip()+
  ggtitle("Impot")
g1

wv <- predict(model, newdata = c("telephone", "internet", "contact"), type = "embedding")
wv <- wv["telephone", ] - wv["internet", ] + wv["contact", ]

predict(model, newdata = wv, type = "nearest", top_n = 10)

```


### Clustering

```{r 02}

#on typologise des termes

library(fastcluster) #pour aller plus vite
distance<-as.dist(1 - cor(t(embedding)))
arbre <- hclust(distance, method = "ward.D2")
plot(arbre,  xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(arbre,16, border = "green3")
group<- as.data.frame(cutree(arbre, k = 16))

group<- group %>% 
  rownames_to_column(var="word")%>%
  rename(group=2)%>%
  left_join(updated_vocab2, by="word") 

foo<- group %>%
  filter(n>100 & n<7000)

library(ggwordcloud)
ggplot(group, aes(label = word, size = n, color=n)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 10) +
  facet_wrap(vars(group), ncol=4)

ggsave("./Images/embedding2.jpeg", width = 28, height = 20, units = "cm")

```

Encore un TSNE

```{r 03}

set.seed(57)
rtsne_out <- Rtsne(distance,
                  initial_dims = 50,
                  perplexity = 50,
                  partial_pca=TRUE,
                  theta=.5,
                  num_threads=4, 
                  verbose=1,
                  check_duplicates = FALSE)

color.vec = c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58", "seagreen1", "seagreen4", "slateblue4", "firebrick", "Royalblue",
              "purple","Orange","Cyan","Coral","Gold","Chartreuse")

#des manip pour associer les groupe du clustering aux termes et à la leur coordonnée dans tsne.
tsne_out1<-as.data.frame(rtsne_out$Y) %>%rename(D1=V1,D2=V2)
tsne_out2<-as.data.frame(cbind(tsne_out1,embedding))%>%
  rownames_to_column(var="word")%>%
  left_join(group)%>%
  filter(n<7000 & n>200)

ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
  geom_text_repel(aes(label=word, 
                      size=log10(n),
                      alpha=log10(n),
                  color=as.factor(group),
                  max.overlap=Inf))+
  theme(legend.position = "none")+
  labs(x=NULL, y=NULL)+  
  scale_size(range = c(.1, 3))

ggsave("./Images/embedding3.jpeg", width = 28, height = 20, units = "cm")

```

# Text as vectors

## Text vectorisation

A text vector is the sum of word vectors.Document vectors are the sum of the vectors of the words which are part of the document standardised by the scale of the vector space. This scale is the sqrt of the average inner product of the vector elements

https://rdrr.io/cran/word2vec/man/doc2vec.html

```{r 04}
#titre<-UD %>%group_by
x      <- data.frame(doc_id           = text2$doc_id, 
                     text             = text2$description, 
                     stringsAsFactors = FALSE)
x$text <- txt_clean_word2vec(x$text, tolower=TRUE)
emb1 <- doc2vec(model, x$text,  split = " ",type = "embedding")


```

## comparison with a concept vector

```{r 05}
## vectorisation des textes

newdoc <- doc2vec(model, "injustice arbitraire inégalité iniquité partialité équité impartialité justice légitimité", type = "embedding")

foo<-word2vec_similarity(emb1, newdoc)
emb2<-cbind(foo,x)%>%
  as.data.frame() %>% arrange(desc(foo))
head(emb2,10)
```


## Working in a common space

Words and texts in the common space of vectors reduced to a 2D projection. 


```{r 06}
vector_tot<-rbind(embedding, emb1)%>%
  as.data.frame()%>%
  drop_na()

tsne_out <- Rtsne(vector_tot,initial_dims = 50,
                  perplexity = 50,
                  partial_pca=TRUE,
                  theta=.5,
                  num_threads=4, 
                  verbose=1,
                  check_duplicates = FALSE) # Run TSNE
tsne_out2<-as.data.frame(tsne_out$Y)
tsne_out2[1:6737,3]<-"word"
tsne_out2[6738:23304,3]<-"doc"
```


à recoriger 

w<-as.data.frame(rownames(emb1))%>%
  rename(tag=1)
d<-as.data.frame(rownames(embedding))%>%rename(tag=1)
x<-rbind(d,w)
tsne_out3<-cbind(tsne_out2,x)
# tsne_out3<-cbind(tsne_out2) %>%   left_join(updated_vocab2) %>%   filter(n>4)
library(ggrepel)
tsne_out3%>%
  ggplot(aes(x=V1, y=V2, label=tag, group=V3))+
  geom_text_repel(aes(label=tag,color=V3),max.overlaps=50, size=2)+
    labs(title="",
       subtitle="",
       x= NULL, y=NULL)+ 
  theme(legend.position = "none") 



# Conclusion 

Some alternatives : Gloves, Fastext

other package `text2vec` `doc2vec` berndt schmidts


Other applications

* ML : as featuring
* 


Some great applications :  @gennaro_emotion_2022 ou  @hamilton_diachronic_2018

The future came from Transformers (that's the last session)

# References

